# NicheRunner API

## Setup

```bash
python -m venv .venv
. .venv/bin/activate
pip install -r requirements.txt
pip install -r requirements-dev.txt
```

## Run

```bash
uvicorn app.main:app --reload
```

Optional (auto-reload on Windows):

```bash
pip install watchfiles==0.24.0
```

## Auth

Login via `POST /auth/login` (JSON body) to receive an HTTP-only session cookie.
Use `POST /auth/logout` to clear it.
Cookie-based requests to non-GET endpoints must include `X-CSRF-Token` matching the `sptx_csrf` cookie.
Use `GET /auth/csrf` if you need to refresh the CSRF token.

## Data input (MVP)

The API expects HPG paths to a prepared bundle (no NIH ingestion in MVP).
Provide the required paths in the run config:
- `cosmx_h5ad_path`
- `reference_h5ad_path`
- `cell_metadata_path`

You can also provide `dataset_id` to resolve paths from the dataset registry.

## Registry endpoints

- `GET /datasets` (optional filters: `organ`, `platform`, `preset_id`)
- `GET /presets` (optional filters: `organ`, `platform`)

Dataset registry file location is controlled by `DATASETS_REGISTRY_PATH`.

## Dataset manifest helper

Use `scripts/generate_dataset_manifest.py` (run in the pipeline conda env) to create a cached schema manifest entry:

```bash
python scripts/generate_dataset_manifest.py \
  --id ibd_cosmx_sample_a \
  --label "IBD CosMx Sample A" \
  --organ colon \
  --platform cosmx \
  --h5ad /path/to/GSE234713_CosMx_combined.h5ad \
  --metadata /path/to/GSE234713_CosMx_cell_metadata.csv.gz \
  --registry registries/datasets.json
```

## Staged runs

Use `stages` in the run config to run multiple pipeline steps in one SLURM job:

```json
{
  "stages": ["cell2loc_nmf", "post_nmf", "rcausal_mgm", "mlp", "report"],
  "post_nmf_mode": "papermill",
  "post_nmf_notebook_path": "pipeline_assets/IBD_Post_NMF_Analysis.ipynb",
  "rcausal_mode": "python",
  "rcausal_script_path": "pipeline_assets/IBD_RCausalMGM_Preparation.py",
  "mlp_script_path": "pipeline_assets/IBD_MLP_44Features.py",
  "report_title": "NicheRunner Paper Run"
}
```

## Preflight

Use `POST /runs/preflight` with the same config payload to validate required fields and paths.
Set `check_paths=false` to skip filesystem checks.

Stage B (`post_nmf`) requires `papermill` installed in the SLURM conda environment.
The RCausalMGM stage (`rcausal_mgm`) defaults to running the notebook with papermill; set `rcausal_mode=python` and `rcausal_script_path` to use a script instead. When using the script, `rcausal_args` defaults to `--output-dir <output_dir>/rcausal_mgm` plus `--niche-h5ad`/`--neighborhood-h5ad` from `rcausal_*_h5ad_path` or `cosmx_h5ad_path`.
The report stage generates `report/report.html`, `report/figures/*`, `report/tables/*`, and `artifacts/manifest.json`; PDF generation uses `pandoc` if available.

Preflight join-key validation (on by default) compares h5ad obs to metadata using `unique_cell_id` or `fov+cell_ID` and returns counts.
Configure with:
- `check_join_keys` (bool, default true)
- `join_key_strategy` (`auto`, `unique_cell_id`, `fov_cell_id`)
- `join_key_delimiter` (default `"__"`)
- `max_missing_fraction` (default 0.0)
- `max_missing_count` (default 0)

Join-key validation requires `anndata==0.10.7` and `pandas` 2.2.x (2.2.2 for Python < 3.13, 2.2.3 for Python >= 3.13), compatible with h5ad files generated by the pipeline's anndata 0.10.x series.

If those dependencies are missing, set `PREFLIGHT_SLURM_FALLBACK=true` to submit a short SLURM preflight job that computes join-key counts.

You can override SLURM resources for fallback checks with `preflight_slurm` in the run config (inherits account/partition/qos from `slurm` if omitted).

Preflight join-key results are cached by `(dataset_id, manifest_hash, preset_id)` for a short TTL to avoid re-reading large h5ad files. Set `PREFLIGHT_CACHE_TTL_SECONDS`.

## Rerun

Use `POST /runs/{run_id}/rerun` with:

```json
{
  "run_name": "new-run-name",
  "submit": true,
  "queue": false
}
```

This reuses the original config, strips runtime-only keys, and regenerates output paths when the prior output lived under the old run directory.

## Environment

- `BASIC_AUTH_USER` / `BASIC_AUTH_PASS` (login credentials; required unless `AUTH_PASSWORD_HASH` is set; do not use `admin`/`admin`)
- `AUTH_PASSWORD_HASH` (optional, pbkdf2_sha256$iterations$salt$hash)
- `SESSION_SECRET` (HMAC secret for session tokens; must be non-default)
- `SESSION_TTL_MINUTES`, `COOKIE_NAME`, `COOKIE_SECURE`, `COOKIE_SAMESITE`
- `CSRF_COOKIE_NAME`, `CSRF_HEADER_NAME`
- `ALLOWED_ORIGINS` (comma-separated)
- `RUNS_DIR`, `PRESETS_DIR`, `DB_PATH`, `ARTIFACT_ROOTS`
- `DATASETS_REGISTRY_PATH`
- `PREFLIGHT_SLURM_FALLBACK`, `PREFLIGHT_SLURM_TIMEOUT_SECONDS`, `PREFLIGHT_SLURM_POLL_SECONDS`
- `PREFLIGHT_CACHE_TTL_SECONDS`
- `QUEUE_ENABLED`, `WORKER_ENABLED` (default false), `WORKER_POLL_SECONDS`
- `RUN_RETENTION_DAYS`, `CLEANUP_INTERVAL_SECONDS`
- `DISK_WARN_FREE_GB`, `DISK_WARN_PERCENT`
- `PREFLIGHT_CHECK_PATHS` (default true)

Set `RUN_RETENTION_DAYS=0` to disable cleanup.

## Worker

Run the queue worker as a separate process:

```bash
python -m app.worker
```

Retention cleanup removes `run_dir` for terminal runs older than `RUN_RETENTION_DAYS` when `output_dir` is outside `run_dir`.
Stage logs are written under `output_dir/logs/`.

## SSH submitter (HiPerGator)

If the API runs on a VM and SLURM submission happens from a login node, see
`docs/SSH_SUBMITTER.md` for the SSH submitter configuration and a smoke test.

## Password hash (optional)

Generate a PBKDF2 hash for `AUTH_PASSWORD_HASH`:

```bash
python - <<'PY'
import os, base64, hashlib
iterations = 200_000
salt = base64.urlsafe_b64encode(os.urandom(12)).decode("ascii").rstrip("=")
password = "change-me"
dk = hashlib.pbkdf2_hmac("sha256", password.encode(), salt.encode(), iterations)
hash_b64 = base64.urlsafe_b64encode(dk).decode("ascii").rstrip("=")
print(f"pbkdf2_sha256${iterations}${salt}${hash_b64}")
PY
```
